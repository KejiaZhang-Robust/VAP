<div align="center">
  <h2 style="font-size: 36px; font-weight: bold; color: #333;">Poison as Cure: Visual Noise for Mitigating Object Hallucinations in LVMs</h2>
  <h4 style="font-size: 20px; color: #777; font-style: italic;">What doesn't kill me makes me stronger</h4>
</div>

<div align="center" style="margin-top: 20px;">
  <!-- Stars Badge with Custom Color -->
  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/KejiaZhang-Robust/VAP?style=social&color=ff6347" style="margin: 0 0px;">
  <!-- Forks Badge with Custom Color -->
  <img alt="GitHub forks" src="https://img.shields.io/github/forks/KejiaZhang-Robust/VAP?style=social&color=1e90ff" style="margin: 0 0px;">
  <!-- arXiv Badge with Custom Color -->
  <a href="https://arxiv.org/abs/2412.00143">
    <img src="https://img.shields.io/badge/arXiv-2412.00143-b31b1b?style=flat-square" alt="arXiv" style="margin: 0 0px;" />
  </a>
  <!-- License Badge with Custom Color -->
  <img alt="GitHub License" src="https://img.shields.io/github/license/KejiaZhang-Robust/VAP?style=flat-square&color=32CD32" style="margin: 0 0px;">
  <!-- Language Badge -->
  <img alt="Language" src="https://img.shields.io/github/languages/top/KejiaZhang-Robust/VAP?style=flat-square&color=9acd32" style="margin: 0 0px;">
</div>

<div align="center" style="margin-top: 30px;">
  <h3 style="font-size: 24px; font-weight: bold; color: #333;">Kejia Zhang, Keda Tao, Jiasheng Tang, Huan Wang</h3>
</div>

<!-- LOGO.png -->
<div align="center" style="margin-top: 20px;">
  <img src="image/LOGO.png" height="100" alt="Westlake University Logo" style="margin-right: 20px; display: inline-block;">
</div>

---

# Code Implementation Overview

## 1. Baseline LVMs Details

We evaluate eight state-of-the-art Large Vision-Language Models (LVMs) to validate the efficacy of our proposed approach. These models, spanning significant advancements in multimodal AI from September 2023 to December 2024, range from 7.1B to 16.1B parameters. The selected models combine cutting-edge language models (e.g., Vicuna, Qwen2, Gemma2) with sophisticated vision encoders such as CLIP, SigLIP, and custom vision transformers. Our focus is on models that address the challenges of hallucinations in vision-language understanding.

| **Model**      | **Parameters** | **Language Model** | **Vision Model** | **Release Date** |
| -------------- | -------------- | ------------------ | ---------------- | ---------------- |
| LLaVA-v1.5     | 7.1B           | Vicuna-7B          | CLIP ViT-L/14    | 2023-09          |
| Instruct-BLIP  | 7.9B           | Vicuna-7B          | ViT-G            | 2023-09          |
| Intern-VL2     | 8.1B           | InternLM2.5-7B     | InternViT-300M   | 2024-07          |
| Intern-VL2-MPO | 8.1B           | InternLM2.5-7B     | InternViT-300M   | 2024-11          |
| DeepSeek-VL2   | 16.1B          | Gemma2-9B          | SigLIP-400M      | 2024-12          |
| Qwen-VL2       | 8.3B           | Qwen2-7B           | ViT-Qwen         | 2024-08          |
| LLaVA-OV       | 8.0B           | Qwen2-7B           | SigLIP-400M      | 2024-08          |
| Ovis1.6-Gemma2 | 9.4B           | Gemma2-9B          | SigLIP-400M      | 2024-11          |

---

## 2. Hallucination Evaluation Dataset

**POPE Benchmark**: Evaluation triplets are generated by sampling from the MS-COCO dataset and are formatted in the following JSON structure:

- [POPE GitHub Repository](https://github.com/RUCAIBox/POPE)
- [Download MS-COCO Dataset](http://images.cocodataset.org/zips/val2014.zip)

Example JSON format:

```json
{
  "id": 0,
  "image": "name", // The image filename from the dataset
  "question": "XXX", // The generated question related to the image
  "gt": "yes/no" // The ground truth answer (binary: "yes" or "no")
}
```

**BEAF Benchmark**: This dataset follows the format specified in the BEAF repository.

- [BEAF Github Repository](https://github.com/postech-ami/BEAF)
- [BEAF Dataset Download](https://drive.google.com/file/d/1Xx7j8Hz8QX3Fl_hpSBet6r15njhwCgeR/view)

---

## 3. Environment Setup

The following dependencies are required to replicate the experiments in this study:

- Python >= 3.7
- PyTorch >= 1.10.0
- Transformers library >= 4.10.0
- Hugging Face Datasets library
- CUDA-compatible GPU for efficient model inference and fine-tuning
- Model weights (available upon request)

Install the necessary libraries with:

```bash
pip install torch transformers datasets

## Running Code

1. LVMs VQA Inference

```

sh script/VQA.sh

```

2. LVMs VQA Inference under VAP vision input.

```

sh script/VAP.sh

```

3. Evaluate Hallucination

```

sh script/evaluate.sh

```

## Setup Environment

### LLAVA-v1.5

- liuhaotian/llava-v1.5-7b [Huggingface Page](https://huggingface.co/liuhaotian/llava-v1.5-7b)

```

cd env_setting/LLaVA
conda create -n llava python=3.10 -y
conda activate llava
pip install --upgrade pip
pip install -e .
pip install ftfy regex tqdm
pip install protobuf
pip install transformers_stream_generator
pip install matplotlib

```

## Instruct-BLIP-7B

- Salesforce/instructblip-vicuna-7b [Huggingface Page](https://huggingface.co/Salesforce/instructblip-vicuna-7b)

## Qwen2-vl

- Qwen/Qwen2-VL-7B-Instruct: [Huggingface Page](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct)

```

conda create -n qwen python==3.11 -y
conda activate qwen
cd env_setting/transformers
pip install .
pip install qwen-vl-utils
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install accelerate==0.26.1
pip install ftfy regex tqdm
pip install matplotlib

```

## InternVL Series

- OpenGVLab/InternVL2-8B [Huggingface Page](https://huggingface.co/OpenGVLab/InternVL2-8B)
- OpenGVLab/InternVL2-8B-MPO [Huggingface Page](https://huggingface.co/OpenGVLab/InternVL2-8B-MPO)

```

conda create -n internvl python=3.9 -y
conda activate internvl
pip install lmdeploy==0.5.3
pip install timm
pip install ftfy regex tqdm
pip install matplotlib
pip install flash-attn==2.3.6 --no-build-isolation

```

## LLaVA-OneVision-7B

- lmms-lab/llava-onevision-qwen2-7b-ov [Huggingface Page](https://huggingface.co/lmms-lab/llava-onevision-qwen2-7b-ov)

```

cd env_setting/LLaVA-NeXT
conda create -n llava_onevision python=3.10 -y
conda activate llava_onevision
pip install --upgrade pip
pip install -e ".[train]"
pip install ftfy regex tqdm
cd ../transformers
pip install .
pip install transformers==4.47.0
pip install flash_attn-2.5.2+cu122torch2.1cxx11abiFALSE-cp310-cp310-linux_x86_64.whl
pip install matplotlib

```

## deepseek-vl2

- deepseek-ai/deepseek-vl2 [Huggingface Page](https://huggingface.co/deepseek-ai/deepseek-vl2)

```

conda create -n deepseek python==3.10 -y
conda activate deepseek
cd env_setting/DeepSeek-VL2/
pip install -e .
pip install ftfy regex tqdm
pip install matplotlib
pip install flash_attn-2.5.8+cu122torch2.3cxx11abiFALSE-cp310-cp310-linux_x86_64.whl
pip install --force-reinstall --no-deps --pre xformers
pip install transformers==4.38.2

```

```
